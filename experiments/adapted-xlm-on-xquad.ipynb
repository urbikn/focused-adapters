{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Fine-tuning mBERT on SQuAD with task adapters"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:45:56.103468Z","iopub.status.busy":"2022-06-11T20:45:56.102841Z","iopub.status.idle":"2022-06-11T20:45:56.724015Z","shell.execute_reply":"2022-06-11T20:45:56.723285Z","shell.execute_reply.started":"2022-06-11T20:45:56.103433Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mmm/anaconda3/envs/aml/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Reusing dataset squad (/home/mmm/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n","100%|██████████| 2/2 [00:00<00:00, 533.86it/s]\n"]}],"source":["from datasets import load_dataset\n","\n","raw_datasets = load_dataset(\"squad\")\n","#raw_datasets['train'] = raw_datasets['train'].shard(num_shards=40, index=0)\n","#raw_datasets['validation'] = raw_datasets['validation'].shard(num_shards=10, index=0)"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing the training dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:45:58.230100Z","iopub.status.busy":"2022-06-11T20:45:58.229458Z","iopub.status.idle":"2022-06-11T20:46:01.462914Z","shell.execute_reply":"2022-06-11T20:46:01.461866Z","shell.execute_reply.started":"2022-06-11T20:45:58.230061Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_checkpoint = \"xlm-roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:46:01.465412Z","iopub.status.busy":"2022-06-11T20:46:01.465018Z","iopub.status.idle":"2022-06-11T20:46:01.479457Z","shell.execute_reply":"2022-06-11T20:46:01.478574Z","shell.execute_reply.started":"2022-06-11T20:46:01.465372Z"},"trusted":true},"outputs":[],"source":["max_length = 512\n","stride = 128\n","\n","def preprocess_training_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    context = examples[\"context\"]\n","    \n","    # Tokenize question and context together into one input,\n","    # they will seperable with a special token between them.\n","    # Tokenizer will also split context into multiple chuncks,\n","    # if the max_length is exceeded.\n","    inputs = tokenizer(\n","        questions,\n","        context,\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # offset_mapping is the result of the split into\n","    # multiple chunks\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    answers = examples[\"answers\"]\n","    start_positions = []\n","    end_positions = []\n","\n","    # This step is required to see if an answer is inside\n","    # the context chunks or not. It labels the multiple chunks\n","    # generated by the tokenizer into either not having\n","    # the answer, or where the answer is located\n","    for i, offset in enumerate(offset_mapping):\n","        sample_idx = sample_map[i]\n","        answer = answers[sample_idx]\n","        start_char = answer[\"answer_start\"][0]\n","        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","        sequence_ids = inputs.sequence_ids(i)\n","\n","        # Find the start and end of the context\n","        idx = 0\n","        while sequence_ids[idx] != 1:\n","            idx += 1\n","        context_start = idx\n","        while sequence_ids[idx] == 1:\n","            idx += 1\n","        context_end = idx - 1\n","\n","        # If the answer is not fully inside the context, label is (0, 0)\n","        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n","            start_positions.append(0)\n","            end_positions.append(0)\n","        else:\n","            # Otherwise it's the start and end token positions\n","            idx = context_start\n","            while idx <= context_end and offset[idx][0] <= start_char:\n","                idx += 1\n","            start_positions.append(idx - 1)\n","\n","            idx = context_end\n","            while idx >= context_start and offset[idx][1] >= end_char:\n","                idx -= 1\n","            end_positions.append(idx + 1)\n","\n","    inputs[\"start_positions\"] = start_positions\n","    inputs[\"end_positions\"] = end_positions\n","    return inputs"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:46:01.481464Z","iopub.status.busy":"2022-06-11T20:46:01.481094Z","iopub.status.idle":"2022-06-11T20:46:01.492634Z","shell.execute_reply":"2022-06-11T20:46:01.491857Z","shell.execute_reply.started":"2022-06-11T20:46:01.481426Z"},"trusted":true},"outputs":[],"source":["def preprocess_validation_examples(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","    context = examples[\"context\"]\n","    \n","    # It's the same thing as with the preprocess_training_examples\n","    # tokenizer, but here \n","    inputs = tokenizer(\n","        questions,\n","        context,\n","        max_length=max_length,\n","        truncation=\"only_second\",\n","        stride=stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n","    example_ids = []\n","\n","    for i in range(len(inputs[\"input_ids\"])):\n","        sample_idx = sample_map[i]\n","        example_ids.append(examples[\"id\"][sample_idx])\n","\n","        sequence_ids = inputs.sequence_ids(i)\n","        offset = inputs[\"offset_mapping\"][i]\n","        inputs[\"offset_mapping\"][i] = [\n","            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n","        ]\n","\n","    inputs[\"example_id\"] = example_ids\n","    return inputs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:46:01.495948Z","iopub.status.busy":"2022-06-11T20:46:01.495589Z","iopub.status.idle":"2022-06-11T20:46:12.897314Z","shell.execute_reply":"2022-06-11T20:46:12.896482Z","shell.execute_reply.started":"2022-06-11T20:46:01.495919Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/mmm/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-958f9bb5656a0ef1.arrow\n","Loading cached processed dataset at /home/mmm/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-b65529454bcbf523.arrow\n"]}],"source":["train_dataset = raw_datasets[\"train\"].map(\n","    preprocess_training_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names,\n",")\n","\n","validation_dataset = raw_datasets[\"validation\"].map(\n","    preprocess_validation_examples,\n","    batched=True,\n","    remove_columns=raw_datasets[\"validation\"].column_names,\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from transformers import default_data_collator\n","\n","train_dataset.set_format(\"torch\")\n","validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n","validation_set.set_format(\"torch\")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n",")\n","\n","validation_dataloader = DataLoader(\n","    validation_dataset,\n","    shuffle=True,\n","    collate_fn=default_data_collator,\n","    batch_size=8,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Computing the metrics"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T21:11:03.671912Z","iopub.status.busy":"2022-06-11T21:11:03.671540Z","iopub.status.idle":"2022-06-11T21:11:06.926391Z","shell.execute_reply":"2022-06-11T21:11:06.925636Z","shell.execute_reply.started":"2022-06-11T21:11:03.671880Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForQuestionAnswering\n","\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["adapter_name = 'squad_adapter'\n","model.add_adapter(adapter_name)\n","# model.load_adapter(\"./adapter_qa_2ep/\")\n","model.train_adapter(adapter_name)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["XLMRobertaForQuestionAnswering(\n","  (shared_parameters): ModuleDict()\n","  (roberta): RobertaModel(\n","    (shared_parameters): ModuleDict()\n","    (invertible_adapters): ModuleDict()\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (prefix_tuning): PrefixTuningShim(\n","                (pool): PrefixTuningPool(\n","                  (prefix_tunings): ModuleDict()\n","                )\n","              )\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (adapters): ModuleDict()\n","              (adapter_fusion_layer): ModuleDict()\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","            (adapters): ModuleDict(\n","              (squad_adapter): Adapter(\n","                (non_linearity): Activation_Function_Class(\n","                  (f): ReLU()\n","                )\n","                (adapter_down): Sequential(\n","                  (0): Linear(in_features=768, out_features=48, bias=True)\n","                  (1): Activation_Function_Class(\n","                    (f): ReLU()\n","                  )\n","                )\n","                (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n","              )\n","            )\n","            (adapter_fusion_layer): ModuleDict()\n","          )\n","        )\n","      )\n","    )\n","    (prefix_tuning): PrefixTuningPool(\n","      (prefix_tunings): ModuleDict()\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model.set_active_adapters(adapter_name)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["batch_size = 8"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from transformers import TrainingArguments"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["model_name = model_checkpoint.split(\"/\")[-1]\n","args = TrainingArguments(\n","    f\"{model_name}-adapter-squad\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=1e-4,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=2,\n","    weight_decay=0.01\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from transformers import AdapterTrainer"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["trainer = AdapterTrainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=validation_dataset,\n","    data_collator=default_data_collator,\n","    tokenizer=tokenizer\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/mmm/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 87872\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 21968\n","  0%|          | 0/21968 [00:00<?, ?it/s]"]},{"ename":"RuntimeError","evalue":"CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 5.93 GiB total capacity; 4.82 GiB already allocated; 70.62 MiB free; 4.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/mmm/Dropbox/Org/Uni/SS22/AML/adapter-based-multilingual-models/experiments/adapted-xlm-on-xquad.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mmm/Dropbox/Org/Uni/SS22/AML/adapter-based-multilingual-models/experiments/adapted-xlm-on-xquad.ipynb#ch0000019?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/trainer.py:1375\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1377\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1378\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1379\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1380\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1381\u001b[0m ):\n\u001b[1;32m   1382\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/trainer.py:1959\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   1958\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 1959\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   1961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1962\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/trainer.py:1991\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1990\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1991\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   1992\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   1993\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1541\u001b[0m, in \u001b[0;36mRobertaForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[39m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1539\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1541\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1542\u001b[0m     input_ids,\n\u001b[1;32m   1543\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1544\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1545\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1546\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1547\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1548\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1549\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1550\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1551\u001b[0m )\n\u001b[1;32m   1553\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1555\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqa_outputs(sequence_output)\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/adapters/context.py:103\u001b[0m, in \u001b[0;36mForwardContext.wrap.<locals>.wrapper_func\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39madapters \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mcls\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 103\u001b[0m         results \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:875\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    866\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    867\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    868\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    871\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    872\u001b[0m )\n\u001b[1;32m    873\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minvertible_adapters_forward(embedding_output)\n\u001b[0;32m--> 875\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    876\u001b[0m     embedding_output,\n\u001b[1;32m    877\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    878\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    879\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    880\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    881\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    882\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    883\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    884\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    885\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    886\u001b[0m )\n\u001b[1;32m    887\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    888\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:544\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    535\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    536\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    537\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    545\u001b[0m         hidden_states,\n\u001b[1;32m    546\u001b[0m         attention_mask,\n\u001b[1;32m    547\u001b[0m         layer_head_mask,\n\u001b[1;32m    548\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    549\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    550\u001b[0m         past_key_value,\n\u001b[1;32m    551\u001b[0m         output_attentions,\n\u001b[1;32m    552\u001b[0m     )\n\u001b[1;32m    554\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    555\u001b[0m (attention_mask,) \u001b[39m=\u001b[39m adjust_tensors_for_parallel(hidden_states, attention_mask)\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:430\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    419\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    420\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    427\u001b[0m ):\n\u001b[1;32m    428\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    431\u001b[0m         hidden_states,\n\u001b[1;32m    432\u001b[0m         attention_mask,\n\u001b[1;32m    433\u001b[0m         head_mask,\n\u001b[1;32m    434\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    435\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    439\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:354\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    346\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    353\u001b[0m ):\n\u001b[0;32m--> 354\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    355\u001b[0m         hidden_states,\n\u001b[1;32m    356\u001b[0m         attention_mask,\n\u001b[1;32m    357\u001b[0m         head_mask,\n\u001b[1;32m    358\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    359\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    360\u001b[0m         past_key_value,\n\u001b[1;32m    361\u001b[0m         output_attentions,\n\u001b[1;32m    362\u001b[0m     )\n\u001b[1;32m    363\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    364\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:279\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    275\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[39m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m attention_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(attention_probs)\n\u001b[1;32m    281\u001b[0m \u001b[39m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n","File \u001b[0;32m~/anaconda3/envs/aml/lib/python3.10/site-packages/torch/nn/functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1279\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 5.93 GiB total capacity; 4.82 GiB already allocated; 70.62 MiB free; 4.88 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in ./adapter_qa_6ep/adapter_config.json\n","Module weights saved in ./adapter_qa_6ep/pytorch_adapter.bin\n","Configuration saved in ./adapter_qa_6ep/head_config.json\n","Module weights saved in ./adapter_qa_6ep/pytorch_model_head.bin\n"]}],"source":["model.save_adapter(\"./adapter_qa_6ep/\", adapter_name)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["adapter_checkpoint = \"./adapter_qa_xlm_4ep/\"\n","adapter_name = 'squad_adapter'\n","model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n","model.load_adapter(adapter_checkpoint)\n","model.set_active_adapters(adapter_name)"]},{"cell_type":"markdown","metadata":{},"source":["## Testing the fine-tuned model"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:21:26.560950Z","iopub.status.busy":"2022-06-11T20:21:26.560574Z","iopub.status.idle":"2022-06-11T20:21:29.335409Z","shell.execute_reply":"2022-06-11T20:21:29.334601Z","shell.execute_reply.started":"2022-06-11T20:21:26.560917Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'score': 0.5330999493598938, 'start': 3, 'end': 15, 'answer': 'Transformers'}"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import pipeline\n","\n","# Replace this with your own checkpoint\n","# model_checkpoint = adapter_checkpoint\n","question_answerer = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n","\n","context = \"\"\"\n","? Transformers is backed by the three most popular deep learning libraries ? Jax, PyTorch and TensorFlow ? with a seamless integration\n","between them. It's straightforward to train your models with one before loading them for inference with the other.\n","\"\"\"\n","question = \"What is backed by deep learning libraries?\"\n","question_answerer(question=question, context=context)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["## Validating using XQuAD"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:24:53.487826Z","iopub.status.busy":"2022-06-11T20:24:53.487448Z","iopub.status.idle":"2022-06-11T20:24:53.492955Z","shell.execute_reply":"2022-06-11T20:24:53.491959Z","shell.execute_reply.started":"2022-06-11T20:24:53.487771Z"},"trusted":true},"outputs":[],"source":["def get_predictions(dataset):\n","    \n","    predictions = []\n","    for example in tqdm(dataset):\n","        question = example['question']\n","        context = example['context']\n","        prediction = question_answerer(question=question, context=context)\n","\n","        predictions.append(prediction)\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:24:53.720009Z","iopub.status.busy":"2022-06-11T20:24:53.719205Z","iopub.status.idle":"2022-06-11T20:24:53.726557Z","shell.execute_reply":"2022-06-11T20:24:53.725792Z","shell.execute_reply.started":"2022-06-11T20:24:53.719972Z"},"trusted":true},"outputs":[],"source":["# Need to convert the variables so that they can be used by the evaluation.compute function\n","def convert_for_evaluation(predictions, examples):\n","    ref = []\n","    pred = []\n","    for i, id in enumerate(examples['id']):\n","        ref.append({\n","            'answers': examples['answers'][i],\n","            'id': examples['id'][i]\n","        })\n","        pred.append({\n","            'prediction_text': predictions[i]['answer'],\n","            'id': examples['id'][i]\n","        })\n","        \n","    return pred, ref"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:24:54.656819Z","iopub.status.busy":"2022-06-11T20:24:54.656456Z","iopub.status.idle":"2022-06-11T20:32:23.276844Z","shell.execute_reply":"2022-06-11T20:32:23.276040Z","shell.execute_reply.started":"2022-06-11T20:24:54.656776Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.en/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 486.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for en\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:01<00:00,  3.95it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.es/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 509.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for es\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:45<00:00,  3.44it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.de/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 738.82it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for de\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:31<00:00,  3.59it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.el/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 687.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for el\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [07:21<00:00,  2.70it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.ru/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 577.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for ru\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:50<00:00,  3.40it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.tr/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 491.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for tr\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:05<00:00,  3.89it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.ar/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 709.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for ar\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:40<00:00,  3.50it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.vi/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 570.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for vi\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:10<00:00,  3.84it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.zh/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 493.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for zh\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [04:42<00:00,  4.21it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.hi/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 422.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for hi\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [06:19<00:00,  3.14it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.ro/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 778.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for ro\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [06:04<00:00,  3.26it/s]\n","Reusing dataset xquad (/home/mmm/.cache/huggingface/datasets/xquad/xquad.th/1.0.0/39e1ff0497cbbfb79bbff61024031c10872bbd7c4fd8bc250207a965c39d3336)\n","100%|██████████| 1/1 [00:00<00:00, 627.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Running predictions for th\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1190/1190 [05:45<00:00,  3.44it/s]\n"]}],"source":["from datasets import load_dataset\n","import evaluate\n","\n","squad_metric = evaluate.load(\"squad\")\n","results = {}\n","\n","languages = [\"en\", \"es\", \"de\", \"el\", \"ru\", \"tr\", \"ar\", \"vi\", \"zh\", \"hi\", \"ro\", \"th\"]\n","for lang in languages:\n","    dataset = load_dataset(\"xquad\", 'xquad.' + lang)['validation']\n","    \n","    print('Running predictions for', lang)\n","    predictions = get_predictions(dataset)\n","\n","    predictions, references = convert_for_evaluation(predictions, dataset) \n","    res = squad_metric.compute(predictions=predictions, references=references)\n","    \n","    results[lang] = res"]},{"cell_type":"markdown","metadata":{},"source":["#### Results for Adapter after 4 Epochs of training"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["{'en': {'exact_match': 70.58823529411765, 'f1': 81.92807378059324},\n"," 'es': {'exact_match': 56.38655462184874, 'f1': 74.44198371555919},\n"," 'de': {'exact_match': 55.378151260504204, 'f1': 71.78426388009316},\n"," 'el': {'exact_match': 53.36134453781513, 'f1': 69.88288466249213},\n"," 'ru': {'exact_match': 55.46218487394958, 'f1': 72.2157329253889},\n"," 'tr': {'exact_match': 51.00840336134454, 'f1': 66.92431239875614},\n"," 'ar': {'exact_match': 40.0, 'f1': 57.85652749543065},\n"," 'vi': {'exact_match': 50.924369747899156, 'f1': 71.26272470130272},\n"," 'zh': {'exact_match': 42.94117647058823, 'f1': 51.761668303685106},\n"," 'hi': {'exact_match': 48.23529411764706, 'f1': 65.4816327282602},\n"," 'ro': {'exact_match': 60.252100840336134, 'f1': 74.66969292814782},\n"," 'th': {'exact_match': 55.54621848739496, 'f1': 65.60099797494749}}"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["results"]},{"cell_type":"markdown","metadata":{},"source":["#### Results for Adapter after 2 Epochs of training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-06-11T20:34:04.860626Z","iopub.status.busy":"2022-06-11T20:34:04.860267Z","iopub.status.idle":"2022-06-11T20:34:04.866766Z","shell.execute_reply":"2022-06-11T20:34:04.865637Z","shell.execute_reply.started":"2022-06-11T20:34:04.860593Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'en': {'exact_match': 68.31932773109244, 'f1': 79.57992577694824},\n"," 'es': {'exact_match': 53.445378151260506, 'f1': 71.89479162287164},\n"," 'de': {'exact_match': 53.109243697478995, 'f1': 69.68981945138748},\n"," 'el': {'exact_match': 51.00840336134454, 'f1': 67.5047974154513},\n"," 'ru': {'exact_match': 53.78151260504202, 'f1': 70.08281167508588},\n"," 'tr': {'exact_match': 47.64705882352941, 'f1': 64.56189143969652},\n"," 'ar': {'exact_match': 40.252100840336134, 'f1': 57.207838767946996},\n"," 'vi': {'exact_match': 50.33613445378151, 'f1': 69.86261372946828},\n"," 'zh': {'exact_match': 41.260504201680675, 'f1': 50.05538579067991},\n"," 'hi': {'exact_match': 47.3109243697479, 'f1': 64.38864468134756},\n"," 'ro': {'exact_match': 58.90756302521008, 'f1': 73.32993999535823},\n"," 'th': {'exact_match': 55.21008403361345, 'f1': 65.07878909139407}}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('aml')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"cbc899a97c8cc5998e86f9df52bd5ee7235fa023cb0251f74c37f5d7e8be5c3a"}}},"nbformat":4,"nbformat_minor":4}
